<h1>
  <center>DATA PIPELINES WITH AIRFLOQ</center>
</h1>




Author: Tianlin He

Date: 5 Nov 2020

Tag: #Udacity #Data Engineering #Airflow #AWS #S3

# Project Overview

A music streaming company, Sparkify, has decided that it is time to introduce more automation and monitoring to their **data warehouse ETL pipelines** with **Apache Airflow**. The goal is to create high grade data pipelines that are **dynamic** and built from reusable tasks, can be monitored, and allow easy **backfills**. They also want to run tests against the  datasets **after** the ETL steps have been executed to catch any discrepancies in the datasets.

As their data engineer, you are tasked with building an ETL pipeline that 

1. extracts the data (format=JSON) from Amazon S3 to Redshift, 
2. create dimensional and facts tables
3. run data quality check with Redshift

We will create a **star schema** optimised for queries on song play analysis, which include a **fact table** and four **dimension tables**:

### Fact table: `songplays` Records in log data associated with song plays in the app

### Dimension tables:

* `users` Users in the app
* songs` Songs in music database` 
* `artists` Artists in music database
* `time` Timestamps of records in log data broken down into specific time units (hour, day, week etc)

# Dataset

You'll be working with two datasets that reside in **S3** with the following link:

- Song data: `s3://udacity-dend/song_data`

- Log data: `s3://udacity-dend/log_data`  


## Song Data

The first dataset is a subset of real data from the [Million Song Dataset](https://labrosa.ee.columbia.edu/millionsong/). Each file is in JSON format and contains metadata about a song and the artist of that song. The files are partitioned by the first three letters of each song's track ID. For example, here are filepaths to two files in this dataset.

```txt
song_data/A/B/C/TRABCEI128F424C983.json
song_data/A/A/B/TRAABJL12903CDCF1A.json
```

And below is an example of what a single song file, TRAABJL12903CDCF1A.json, looks like:

```json
{"num_songs": 1, "artist_id": "ARJIE2Y1187B994AB7", "artist_latitude": null, "artist_longitude": null, "artist_location": "", "artist_name": "Line Renaud", "song_id": "SOUPIRU12A6D4FA1E1", "title": "Der Kleine Dompfaff", "duration": 152.92036, "year": 0}
```

## Log Data

The log data are in JSON format files generated by this [event simulator](https://github.com/Interana/eventsim) based on the songs in the dataset above. The log files are partitioned by year and month. For example, here are filepaths to two files in this dataset:

```txt
log_data/2018/11/2018-11-12-events.json
log_data/2018/11/2018-11-13-events.json
```

# Project Development

The project is developed in a **local enviornment (MacOS Catalina)** and based on libraries including  `apache airflow`.

## Setting of AWS

1. On **AWS console**, create **IAM user**
    
    * save the access key and secrete in `dl.cfg`
    
2. On **AWS console**, create a **Redshift cluster**
    * save the cluster endpoint, password, and port name

    	![img](img/cluster-details.png)

    

## Installing and running Airflow locally

### 1. Quick installation by pip

```bash
# airflow needs a home, ~/airflow is the default,
# but you can lay foundation somewhere else if you prefer
# (optional)
$ export AIRFLOW_HOME=~/airflow

# install from pypi using pip
# set dependency contrains because of marshmallow and attrs version issue
$ pip install apache-airflow==1.10.12  --constraint "https://raw.githubusercontent.com/apache/airflow/constraints-1.10.12/constraints-3.7.txt"

# initialize the database
$ airflow initdb
```

### 2. Start Airflow WebUI

```bash
# start the web server, default port is 8080
$ airflow webserver -p 8080

# on a separate Terminal, start the scheduler
$ airflow scheduler
```

Type **localhost:8080** in any browser (e.g. Safari, Chrome)

### 3. Set up connections with AWS

On WebUI page, click Admin -> connectiony -> create connections. You need to create **two** connections:

1. create connections to AWS:
	- **Conn Id**: Enter `aws_credentials`.
	- **Conn Type**: Enter `Amazon Web Services`.
	- **Login**: Enter your **Access key ID** from the IAM User credentials you downloaded earlier.
	- **Password**: Enter your **Secret access key** from the IAM User credentials you downloaded earlier.

2. create connections to AWS Redshift:
	* **Conn Id**: Enter `redshift`.
	* **Conn Type**: Enter `Postgres`.
	* **Host**: Enter the endpoint of your Redshift cluster, excluding the port at the end. You can find this by selecting your cluster in the **Clusters** page of the Amazon Redshift console. See where this is located in the screenshot below. IMPORTANT: Make sure to **NOT** include the port at the end of the Redshift endpoint string.
	* **Schema**: Enter `dev`. This is the Redshift database you want to connect to.
	* **Login**: Enter `awsuser`.
	* **Password**: Enter the password you created when launching your Redshift cluster.
	* **Port**: Enter `5439`.

## Files

We have the following hierarchy of files:

```
---dags
	|---etl_dag.py
	|---create_tables.py
---plugins
	|---__init__.py
	|---operators
	|        |---__init__.py
	|        |---stage_redshift.py
	|        |---load_fact.py
	|        |---load_dimension.py
	|        |---data_quality.py
	|---helpers
	         |---__inint__.py
	         |---sql_queries.py
```

### dags

* `etl_dag.py`: pipeline of dags. They should be run as:

![Final DAG](img/example-dag.png)

* `create_tables.py`: sql queries for creating empty tables

### plugins

#### operators: four airflow operators for implementing of tasks

* `stage_redshift.py`

The stage operator is expected to be able to load any JSON formatted files from S3 to Amazon Redshift. Here, we load the log and song datasets.

* `load_fact.py` and `load_dimension.py`

We utilize them together with provided SQL helper class to run data transformations. Dimension loads are often done with the truncate-insert pattern (where the target table is emptied before the load);  fact tables only allow append-type functionality.

* `data_quality.py`

With this operator, we run four checks on the data quality in tables:

1. If we have non-zero number of rows 
2. If we have any null primary key of each table
3. If we have distinct primary key for the dimension tables
4. If all time records in `time` table are positive

### helpers: sql queries of table transformation, implemented by operators

## 
